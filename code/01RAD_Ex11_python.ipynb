{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrystofKlazek/RAD/blob/main/code/01RAD_Ex11_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJA5t5UlK9k"
      },
      "source": [
        "# 01RAD Exercise 11 - Hands on\n",
        "\n",
        "Authors: Your names here  \n",
        "Date: 2025-12-10  \n",
        "\n",
        "---\n",
        "\n",
        "## Task Description\n",
        "\n",
        "The dataset is based on the **House Sales in King County, USA** dataset, which can be found, for example, on kaggle.com or in the `moderndive` library under the name `house_prices`. The original dataset contains house sale prices in the King County area, which includes Seattle, and the data was collected between May 2014 and May 2015. For our purposes, several variables have been removed, and the dataset has been significantly reduced and slightly modified.\n",
        "\n",
        "The dataset has already been split into three parts and modified, all of which will be used progressively throughout this assignment.\n",
        "\n",
        "---\n",
        "\n",
        "## Variables Description\n",
        "\n",
        "The dataset contains the following 18 variables, and our goal is to explore the influence of 12 of them on the target variable `price`.\n",
        "\n",
        "| Feature         | Description                                           |\n",
        "|------------------|-------------------------------------------------------|\n",
        "| `id`            | Unique identifier for a house                         |\n",
        "| `price`         | Sale price (prediction target)                        |\n",
        "| `bedrooms`      | Number of bedrooms                                    |\n",
        "| `bathrooms`     | Number of bathrooms                                   |\n",
        "| `sqft_living`   | Square footage of the home                            |\n",
        "| `sqft_lot`      | Square footage of the lot                             |\n",
        "| `floors`        | Total number of floors (levels) in the house          |\n",
        "| `waterfront`    | Whether the house has a waterfront view               |\n",
        "| `view`          | Number of times the house has been viewed             |\n",
        "| `condition`     | Overall condition of the house                        |\n",
        "| `grade`         | Overall grade given to the housing unit               |\n",
        "| `sqft_above`    | Square footage of the house apart from the basement   |\n",
        "| `sqft_basement` | Square footage of the basement                        |\n",
        "| `yr_built`      | Year the house was built                              |\n",
        "| `yr_renovated`  | Year when the house was renovated                     |\n",
        "| `sqft_living15` | Living room area in 2015 (after renovations)          |\n",
        "| `sqft_lot15`    | Lot size in 2015 (after renovations)                  |\n",
        "| `split`         | Splitting variable with train, test, and validation samples |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4IQE-BBl5S5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/francji1/01RAD/main/data/01RAD_2024_house.csv\"\n",
        "house_rad = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "house_rad.head()\n",
        "\n",
        "# Convenience splits for later\n",
        "train = house_rad.query('split == \"train\"').copy()\n",
        "test  = house_rad.query('split == \"test\"').copy()\n",
        "val   = house_rad.query('split == \"validation\"').copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYcRDfIllU7"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Exploratory and Graphical Analysis\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Verify the dimensions of the dataset, the types of individual variables, and summarize the basic descriptive statistics of all variables. Plot a histogram and a density estimate for the target variable `price`. Can anything be inferred for future analysis?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2\n",
        "\n",
        "Are all variables usable for analysis and prediction of house prices? If the data contains missing values (or strange or nonsensical observations), can they be imputed (corrected), or must they be removed from the dataset?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3\n",
        "\n",
        "For the selected variables (`price`, `sqft_living`, `grade`, `yr_built`), verify whether the split into train, test, and validation datasets was random. That is, do these variables have approximately the same distributions across the train, test, and validation groups?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ftafwcMFdB2"
      },
      "source": [
        "Consider to transform price (e.g. log‑price) and key size variables if distributions are highly skewed and compare residual diagnostics for transformed vs untransformed models later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ_MVRHbmmnO"
      },
      "outputs": [],
      "source": [
        "# Dimensions\n",
        "print(house_rad.shape)\n",
        "print(house_rad.apply(len).unique()) #This comes out fine - all cols are the same length\n",
        "\n",
        "print(house_rad.dtypes)\n",
        "\n",
        "print(house_rad.describe().T)\n",
        "\n",
        "print(house_rad['price'].describe())\n",
        "\n",
        "low_price = house_rad[house_rad['price']==0]\n",
        "print(low_price)\n",
        "house_rad['log_price'] = np.log(house_rad['price'])\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(house_rad['log_price'], bins=50)\n",
        "plt.title(\"log_rice histogram\")\n",
        "plt.xlabel(\"log_price\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "house_rad = house_rad.copy()\n",
        "\n",
        "house_rad['house_age'] = np.where(\n",
        "    house_rad['yr_renovated'] != 0,\n",
        "    2015 - house_rad['yr_renovated'],       # renovated → age since renovation\n",
        "    2015 - house_rad['yr_built']            # not renovated → age since built\n",
        ")\n",
        "\n",
        "house_rad = house_rad.drop(columns=['yr_renovated'])\n",
        "house_rad = house_rad.drop(index=[148, 48])\n",
        "house_rad = house_rad.query(\"grade <= 13\").copy()\n"
      ],
      "metadata": {
        "id": "HamKph3gUtfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_rad['split'].value_counts()\n"
      ],
      "metadata": {
        "id": "H-Wc9Vp7VadC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars_q3 = ['price', 'sqft_living', 'grade', 'yr_built', 'log_price']\n",
        "\n",
        "summary_by_split = (\n",
        "    house_rad\n",
        "    .groupby('split')[vars_q3]\n",
        "    .describe()\n",
        "    .round(2)\n",
        ")\n",
        "\n",
        "summary_by_split\n"
      ],
      "metadata": {
        "id": "fnZpCh0GVhYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = house_rad.query('split == \"train\"')\n",
        "test  = house_rad.query('split == \"test\"')\n",
        "val   = house_rad.query('split == \"validation\"')\n",
        "\n",
        "for v in vars_q3:\n",
        "    plt.figure()\n",
        "    plt.hist(train[v].dropna(), bins=40, alpha=0.4, label='train')\n",
        "    plt.hist(test[v].dropna(),  bins=40, alpha=0.4, label='test')\n",
        "    plt.hist(val[v].dropna(),   bins=40, alpha=0.4, label='validation')\n",
        "    plt.title(f\"{v} distribution by split\")\n",
        "    plt.xlabel(v)\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jwqV6sOpVkda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmyvXCcamkp6"
      },
      "source": [
        "\n",
        "## Linear Model (Use Only Training Data, i.e., split == \"train\")\n",
        "\n",
        "### Question 4\n",
        "\n",
        "Calculate the correlations between the regressors and visualize them. Also, compute the condition number (Kappa) and the variance inflation factor (VIF). If multicollinearity is present, decide which variables to use and justify your choices.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 5\n",
        "\n",
        "Using only the training data (split == \"train\") and all selected variables, find a suitable linear regression model that best predicts the target variable `price`, i.e., minimizes the mean squared error (MSE). Compare the VIF and Kappa values of the final model to those of the original regressor matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 6\n",
        "\n",
        "For your selected model from the previous question, calculate the relevant influence measures. Provide the `id` for the top 20 observations with the highest values of DIFFITS and DFBetas, the highest leverage (hat values), and the highest Cook’s distance (i.e., 3 sets of 20 values). Which observations do you consider influential or outliers?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 7\n",
        "\n",
        "Validate the model graphically using residual plots (Residual vs. Fitted, QQ-plot, Cook’s distance, leverage, etc.). Based on this and the previous question, did you identify any suspicious observations in the data that might have resulted from dataset adjustments? Would you recommend removing these observations from the data?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4X4_Gnc80op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69VHJntQrlJV"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import seaborn as sns\n",
        "import patsy\n",
        "\n",
        "train = house_rad.query('split == \"train\"').copy()\n",
        "test  = house_rad.query('split == \"test\"').copy()\n",
        "val   = house_rad.query('split == \"validation\"').copy()\n",
        "\n",
        "exclude = {'price', 'split', 'id', 'Unnamed: 0', 'log_price'}\n",
        "exclude = {c for c in exclude if c in train.columns}\n",
        "\n",
        "num_cols = train.select_dtypes(include='number').columns\n",
        "regressors = [c for c in num_cols if c not in exclude]\n",
        "\n",
        "regressors\n",
        "\n",
        "formula_full = \"log_price ~ \" + \" + \".join(regressors)\n",
        "formula_full\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr_reg = train[regressors].corr()\n",
        "\n",
        "corr_reg.round(3).head()"
      ],
      "metadata": {
        "id": "BYL0ZsbqV3nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_reg, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation matrix of regressors (train)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4vUy3E8XV8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, X = patsy.dmatrices(formula_full, data=train, return_type='dataframe')\n",
        "\n",
        "# Condition number of X (includes intercept)\n",
        "kappa = np.linalg.cond(X)\n",
        "kappa"
      ],
      "metadata": {
        "id": "GDD4xT1KWRct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STANDARDIZED FOR BETTER RESULTS\n",
        "X_no_intercept = X.drop(columns=['Intercept'], errors='ignore')\n",
        "\n",
        "X_std = (X_no_intercept - X_no_intercept.mean()) / X_no_intercept.std(ddof=0)\n",
        "\n",
        "kappa_std = np.linalg.cond(X_std)\n",
        "kappa_std\n"
      ],
      "metadata": {
        "id": "j0wvWUHxWaLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute VIF for each column in X\n",
        "vif_df = pd.DataFrame({\n",
        "    'term': X.columns,\n",
        "    'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "}).sort_values('VIF', ascending=False)\n",
        "\n",
        "vif_df\n"
      ],
      "metadata": {
        "id": "IJQqI4NuWTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = house_rad.query('split == \"train\"').copy()\n",
        "test  = house_rad.query('split == \"test\"').copy()\n",
        "val   = house_rad.query('split == \"validation\"').copy()\n",
        "\n",
        "\n",
        "exclude = {'price', 'split', 'id', 'Unnamed: 0', 'log_price'}\n",
        "exclude = {c for c in exclude if c in train.columns}\n",
        "\n",
        "num_cols = train.select_dtypes(include='number').columns\n",
        "regressors = [c for c in num_cols if c not in exclude]\n",
        "\n",
        "regressors\n",
        "\n",
        "formula_original = \"log_price ~ \" + \" + \".join(regressors)\n",
        "formula_original\n",
        "\n",
        "exclude = {'price', 'split', 'id', 'Unnamed: 0', 'sqft_above', 'sqft_lot', 'sqft_living', 'yr_renovated', 'yr_built', 'log_price'}\n",
        "exclude = {c for c in exclude if c in train.columns}\n",
        "\n",
        "num_cols = train.select_dtypes(include='number').columns\n",
        "regressors = [c for c in num_cols if c not in exclude]\n",
        "\n",
        "regressors\n",
        "\n",
        "formula_full = \"log_price ~ \" + \" + \".join(regressors)\n",
        "formula_full\n",
        "\n",
        "\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def kappa_of_formula(formula, data):\n",
        "    y, X = patsy.dmatrices(formula, data=data, return_type='dataframe')\n",
        "    kappa_raw = np.linalg.cond(X)\n",
        "\n",
        "    # scaled version (more meaningful for multicollinearity)\n",
        "    X_no_int = X.drop(columns=['Intercept'], errors='ignore')\n",
        "    X_std = (X_no_int - X_no_int.mean()) / X_no_int.std(ddof=0)\n",
        "    kappa_std = np.linalg.cond(X_std)\n",
        "\n",
        "    return kappa_raw, kappa_std, X\n",
        "\n",
        "def vif_of_X(X):\n",
        "    vif_df = pd.DataFrame({\n",
        "        'term': X.columns,\n",
        "        'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    }).sort_values('VIF', ascending=False)\n",
        "    return vif_df\n"
      ],
      "metadata": {
        "id": "8ZIYBa4RWkZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Fit both models on TRAIN\n",
        "m_orig  = smf.ols(formula_original, data=train).fit()\n",
        "m_final = smf.ols(formula_full,     data=train).fit()\n",
        "\n",
        "# 2) Training MSE\n",
        "mse_orig  = mse(train['log_price'], m_orig.predict(train))\n",
        "mse_final = mse(train['log_price'], m_final.predict(train))\n",
        "\n",
        "rmse_orig  = np.sqrt(mse_orig)\n",
        "rmse_final = np.sqrt(mse_final)\n",
        "\n",
        "# 3) Kappa + VIF for each\n",
        "\n",
        "# Original\n",
        "kappa_orig_raw, kappa_orig_std, X_orig = kappa_of_formula(formula_original, train)\n",
        "vif_orig = vif_of_X(X_orig)\n",
        "max_vif_orig = vif_orig.query(\"term != 'Intercept'\")['VIF'].max()\n",
        "\n",
        "# Final\n",
        "kappa_final_raw, kappa_final_std, X_final = kappa_of_formula(formula_full, train)\n",
        "vif_final = vif_of_X(X_final)\n",
        "max_vif_final = vif_final.query(\"term != 'Intercept'\")['VIF'].max()\n",
        "\n",
        "# 4) Side-by-side comparison\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'model': 'original',\n",
        "        'train_mse': mse_orig,\n",
        "        'train_rmse': rmse_orig,\n",
        "        'kappa_raw': kappa_orig_raw,\n",
        "        'kappa_std': kappa_orig_std,\n",
        "        'max_vif_non_intercept': max_vif_orig\n",
        "    },\n",
        "    {\n",
        "        'model': 'final',\n",
        "        'train_mse': mse_final,\n",
        "        'train_rmse': rmse_final,\n",
        "        'kappa_raw': kappa_final_raw,\n",
        "        'kappa_std': kappa_final_std,\n",
        "        'max_vif_non_intercept': max_vif_final\n",
        "    }\n",
        "])\n",
        "\n",
        "comparison\n"
      ],
      "metadata": {
        "id": "zo3zfCnkXEck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "\n",
        "# Influence object\n",
        "infl = m_final.get_influence()\n",
        "\n",
        "# DFFITS\n",
        "dffits, dffits_p = infl.dffits\n",
        "\n",
        "# DFBetas: array shape (n_obs, k_params)\n",
        "dfbetas = infl.dfbetas\n",
        "\n",
        "# Leverage (hat values)\n",
        "hat = infl.hat_matrix_diag\n",
        "\n",
        "# Cook's distance\n",
        "cooks, cooks_p = infl.cooks_distance\n",
        "\n",
        "\n",
        "\n",
        "# Build a diagnostic DataFrame indexed like train\n",
        "diag = train[['id']].copy()\n",
        "diag['dffits'] = dffits\n",
        "diag['hat'] = hat\n",
        "diag['cooks'] = cooks\n",
        "\n",
        "# Max absolute DFBetas over all coefficients for each observation\n",
        "diag['max_dfbeta'] = np.abs(dfbetas).max(axis=1)\n",
        "\n",
        "# Absolute values for ranking\n",
        "diag['abs_dffits'] = np.abs(diag['dffits'])\n",
        "diag['abs_max_dfbeta'] = np.abs(diag['max_dfbeta'])\n"
      ],
      "metadata": {
        "id": "PgeFej5N83HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_dffits = diag.nlargest(20, 'abs_dffits')[['id', 'dffits']]\n",
        "top20_dfbeta = diag.nlargest(20, 'abs_max_dfbeta')[['id', 'max_dfbeta']]\n",
        "top20_hat    = diag.nlargest(20, 'hat')[['id', 'hat']]\n",
        "top20_cooks  = diag.nlargest(20, 'cooks')[['id', 'cooks']]\n",
        "\n",
        "print(\"Top 20 by |DFFITS|:\")\n",
        "print(top20_dffits)\n",
        "\n",
        "print(\"\\nTop 20 by max |DFBeta|:\")\n",
        "print(top20_dfbeta)\n",
        "\n",
        "print(\"\\nTop 20 by leverage (hat):\")\n",
        "print(top20_hat)\n",
        "\n",
        "print(\"\\nTop 20 by Cook's distance:\")\n",
        "print(top20_cooks)\n"
      ],
      "metadata": {
        "id": "1iNgc7J686SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = m_final.nobs\n",
        "p = m_final.df_model + 1\n",
        "\n",
        "# Rules of thumb\n",
        "dffits_cut = 2 * np.sqrt(p / n)\n",
        "cooks_cut  = 4 / n\n",
        "hat_cut    = 2 * p / n     # or 3*p/n\n",
        "dfbeta_cut = 2 / np.sqrt(n)\n",
        "\n",
        "dffits_cut, cooks_cut, hat_cut, dfbeta_cut\n"
      ],
      "metadata": {
        "id": "gbVGKk8L9CfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Flag observations\n",
        "diag['flag_dffits'] = diag['abs_dffits']       > dffits_cut\n",
        "diag['flag_cooks']  = diag['cooks']            > cooks_cut\n",
        "diag['flag_hat']    = diag['hat']              > hat_cut\n",
        "diag['flag_dfbeta'] = diag['abs_max_dfbeta']   > dfbeta_cut\n",
        "\n",
        "flag_cols = ['flag_dffits','flag_cooks','flag_hat','flag_dfbeta']\n",
        "\n",
        "# total number of flags per observation\n",
        "diag['n_flags'] = diag[flag_cols].sum(axis=1)\n",
        "\n",
        "# 2) Rows suspicious in at least one sense (or use >=2 if you want stricter)\n",
        "suspects = diag[diag['n_flags'] >= 1].copy()\n",
        "\n",
        "# sort by how many flags, then Cook's D, then |DFFITS|\n",
        "suspects = suspects.sort_values(\n",
        "    ['n_flags','cooks','abs_dffits'],\n",
        "    ascending=[False, False, False]\n",
        ")\n",
        "\n",
        "suspects.head(30)[['id','dffits','max_dfbeta','hat','cooks','n_flags']]\n",
        "\n",
        "suspects = diag[diag['n_flags'] >= 3].copy()\n",
        "\n",
        "# 3) Inspect the underlying data for those suspicious ids\n",
        "suspects_ids = suspects['id'].tolist()\n",
        "\n",
        "cols_to_inspect = [\n",
        "    'id', 'price', 'sqft_living', 'sqft_lot',\n",
        "    'bedrooms', 'bathrooms', 'grade',\n",
        "    'house_age', 'view', 'waterfront'\n",
        "]\n",
        "\n",
        "train.loc[train['id'].isin(suspects_ids), cols_to_inspect] \\\n",
        "     .sort_values('price', ascending=False)\n"
      ],
      "metadata": {
        "id": "vwmqJfuj9Oh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_final = smf.ols(formula_full,     data=train).fit()\n",
        "\n",
        "\n",
        "# Residuals and fitted values from the final model\n",
        "resid   = m_final.resid\n",
        "fitted  = m_final.fittedvalues\n",
        "\n",
        "# 1) Residuals vs Fitted\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(fitted, resid, alpha=0.5)\n",
        "plt.axhline(0, color='black', linewidth=1)\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Fitted (train)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "infl = m_final.get_influence()\n",
        "resid_stud = infl.resid_studentized_internal\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sm.qqplot(resid_stud, line='45', ax=fig.add_subplot(111))\n",
        "plt.title(\"QQ-plot of studentized residuals (train)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "infl = m_final.get_influence()\n",
        "leverage = infl.hat_matrix_diag\n",
        "stud_resid = infl.resid_studentized_external\n",
        "cooks = infl.cooks_distance[0]\n",
        "\n",
        "inf_df = pd.DataFrame({\n",
        "    'id': train['id'].values,\n",
        "    'leverage': leverage,\n",
        "    'stud_resid': stud_resid,\n",
        "    'cooks': cooks\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.scatterplot(\n",
        "    data=inf_df,\n",
        "    x='leverage',\n",
        "    y='stud_resid',\n",
        "    size='cooks',\n",
        "    hue='cooks',\n",
        "    palette='viridis',\n",
        "    alpha=0.7,\n",
        "    legend=False,\n",
        ")\n",
        "\n",
        "# mark a leverage cutoff\n",
        "n = m_final.nobs\n",
        "p = m_final.df_model + 1\n",
        "hat_cut = 2 * p / n\n",
        "ax.axvline(hat_cut, color='red', linestyle='--')\n",
        "\n",
        "ax.set_xlabel(\"Leverage\")\n",
        "ax.set_ylabel(\"Studentized residuals\")\n",
        "ax.set_title(\"Influence plot (size/color = Cook's distance)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Xcu_0t1r9xAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the influence diagnostics and residual plots from Questions 6 and 7, I inspected the underlying covariates of the most influential observations. Most of the high-price points correspond to large, high-grade houses with big lots and sometimes views or waterfront, so they appear to be genuine luxury properties.\n",
        "\n",
        "However, some cases stand out as suspicious: in particular ids 100, 200, 300 and 109 have extremely high prices (4.45–8.5 million) despite modest living area, small lots, low/medium grades, and no view or waterfront, and ids 215 and 119 have extremely low prices (84 000 and 28 500) for fairly ordinary houses. These combinations are inconsistent with the rest of the dataset and likely reflect dataset adjustments or data-entry errors rather than genuine sales.\n",
        "\n",
        "Based on this inspection, I would treat ids 100, 200, 300, 109, 215 and 119 as suspicious outliers caused by data issues and I would recommend removing them before fitting the final model. The remaining influential observations look like valid but extreme properties, so I keep them in the data and simply note that they have a noticeable influence on the fitted regression."
      ],
      "metadata": {
        "id": "CNuA472hhgHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IDs to drop (suspicious / likely bad)\n",
        "bad_ids = [100, 200, 300, 109, 215, 119]\n",
        "\n",
        "# Drop them from the full dataset\n",
        "house_rad = house_rad[~house_rad['id'].isin(bad_ids)].copy()\n",
        "\n",
        "# Recreate splits so train/test/val reflect the change\n",
        "train = house_rad.query('split == \"train\"').copy()\n",
        "test  = house_rad.query('split == \"test\"').copy()\n",
        "val   = house_rad.query('split == \"validation\"').copy()\n",
        "\n",
        "# Optional: quick check they are really gone\n",
        "house_rad[house_rad['id'].isin(bad_ids)]\n"
      ],
      "metadata": {
        "id": "3ok6P-fNh3h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m_final = smf.ols(formula_full,     data=train).fit()\n",
        "\n",
        "# Residuals and fitted values from the final model\n",
        "resid   = m_final.resid\n",
        "fitted  = m_final.fittedvalues\n",
        "\n",
        "\n",
        "# Influence object\n",
        "infl = m_final.get_influence()\n",
        "\n",
        "# DFFITS\n",
        "dffits, dffits_p = infl.dffits\n",
        "\n",
        "# DFBetas: array shape (n_obs, k_params)\n",
        "dfbetas = infl.dfbetas\n",
        "\n",
        "# Leverage (hat values)\n",
        "hat = infl.hat_matrix_diag\n",
        "\n",
        "# Cook's distance\n",
        "cooks, cooks_p = infl.cooks_distance\n",
        "\n",
        "# 1) Flag observations\n",
        "diag['flag_dffits'] = diag['abs_dffits']       > dffits_cut\n",
        "diag['flag_cooks']  = diag['cooks']            > cooks_cut\n",
        "diag['flag_hat']    = diag['hat']              > hat_cut\n",
        "diag['flag_dfbeta'] = diag['abs_max_dfbeta']   > dfbeta_cut\n",
        "\n",
        "flag_cols = ['flag_dffits','flag_cooks','flag_hat','flag_dfbeta']\n",
        "\n",
        "# total number of flags per observation\n",
        "diag['n_flags'] = diag[flag_cols].sum(axis=1)\n",
        "\n",
        "# 2) Rows suspicious in at least one sense (or use >=2 if you want stricter)\n",
        "suspects = diag[diag['n_flags'] >= 1].copy()\n",
        "\n",
        "# sort by how many flags, then Cook's D, then |DFFITS|\n",
        "suspects = suspects.sort_values(\n",
        "    ['n_flags','cooks','abs_dffits'],\n",
        "    ascending=[False, False, False]\n",
        ")\n",
        "\n",
        "suspects.head(30)[['id','dffits','max_dfbeta','hat','cooks','n_flags']]\n",
        "\n",
        "suspects = diag[diag['n_flags'] >= 3].copy()\n",
        "\n",
        "# 3) Inspect the underlying data for those suspicious ids\n",
        "suspects_ids = suspects['id'].tolist()\n",
        "\n",
        "cols_to_inspect = [\n",
        "    'id', 'price', 'sqft_living', 'sqft_lot',\n",
        "    'bedrooms', 'bathrooms', 'grade',\n",
        "    'house_age', 'view', 'waterfront'\n",
        "]\n",
        "\n",
        "train.loc[train['id'].isin(suspects_ids), cols_to_inspect] \\\n",
        "     .sort_values('price', ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build a diagnostic DataFrame indexed like train\n",
        "diag = train[['id']].copy()\n",
        "diag['dffits'] = dffits\n",
        "diag['hat'] = hat\n",
        "diag['cooks'] = cooks\n",
        "\n",
        "# Max absolute DFBetas over all coefficients for each observation\n",
        "diag['max_dfbeta'] = np.abs(dfbetas).max(axis=1)\n",
        "\n",
        "# Absolute values for ranking\n",
        "diag['abs_dffits'] = np.abs(diag['dffits'])\n",
        "diag['abs_max_dfbeta'] = np.abs(diag['max_dfbeta'])\n",
        "\n",
        "\n",
        "# 1) Residuals vs Fitted\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(fitted, resid, alpha=0.5)\n",
        "plt.axhline(0, color='black', linewidth=1)\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Fitted (train)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# studentized internal residuals\n",
        "infl = m_final.get_influence()\n",
        "resid_stud = infl.resid_studentized_internal\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "sm.qqplot(resid_stud, line='45', ax=fig.add_subplot(111))\n",
        "plt.title(\"QQ-plot of studentized residuals (train)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "infl = m_final.get_influence()\n",
        "leverage = infl.hat_matrix_diag\n",
        "stud_resid = infl.resid_studentized_external\n",
        "cooks = infl.cooks_distance[0]\n",
        "\n",
        "inf_df = pd.DataFrame({\n",
        "    'id': train['id'].values,\n",
        "    'leverage': leverage,\n",
        "    'stud_resid': stud_resid,\n",
        "    'cooks': cooks\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.scatterplot(\n",
        "    data=inf_df,\n",
        "    x='leverage',\n",
        "    y='stud_resid',\n",
        "    size='cooks',\n",
        "    hue='cooks',\n",
        "    palette='viridis',\n",
        "    alpha=0.7,\n",
        "    legend=False,\n",
        ")\n",
        "\n",
        "# mark a leverage cutoff\n",
        "n = m_final.nobs\n",
        "p = m_final.df_model + 1\n",
        "hat_cut = 2 * p / n\n",
        "ax.axvline(hat_cut, color='red', linestyle='--')\n",
        "\n",
        "ax.set_xlabel(\"Leverage\")\n",
        "ax.set_ylabel(\"Studentized residuals\")\n",
        "ax.set_title(\"Influence plot (size/color = Cook's distance)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qMoSjlkLh7x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGfHy5Gwrkp9"
      },
      "source": [
        "\n",
        "## Robust Approach\n",
        "\n",
        "### Question 8\n",
        "\n",
        "If you decided to remove any observations from the dataset, work with the filtered dataset, retrain the model from Question 5, and calculate the $R^2$ statistic and MSE on both the training and test data (split == \"test\").\n",
        "\n",
        "---\n",
        "\n",
        "### Question 9\n",
        "\n",
        "Estimate the regression coefficients using a robust Total Least Squares (TLS), on both the filtered dataset (after removing any observations, if applicable) and the original full dataset. Compare the results and discuss any differences in the estimated coefficients and model performance. What insights can you draw about the impact of filtering observations on model robustness?\n",
        "\n",
        "\n",
        "---\n",
        "### Question 10\n",
        "\n",
        "Select the final model and compare the MSE and $R^2$ on the training, test, and validation datasets. What do these values suggest about the quality of the model and potential overfitting? Is your model suitable for predicting house prices in the King County area? If so, does this prediction have any limitations?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  r2_score\n",
        "\n",
        "target = 'log_price'\n",
        "\n",
        "# predictions\n",
        "pred_train = m_final.predict(train)\n",
        "pred_test  = m_final.predict(test)\n",
        "\n",
        "# metrics\n",
        "mse_train = mse(train[target], pred_train)\n",
        "mse_test  = mse(test[target],  pred_test)\n",
        "\n",
        "r2_train  = r2_score(train[target], pred_train)\n",
        "r2_test   = r2_score(test[target],  pred_test)\n",
        "\n",
        "metrics_q8 = pd.DataFrame([\n",
        "    {'split': 'train', 'MSE': mse_train, 'RMSE': np.sqrt(mse_train), 'R2': r2_train},\n",
        "    {'split': 'test',  'MSE': mse_test,  'RMSE': np.sqrt(mse_test),  'R2': r2_test},\n",
        "])\n",
        "\n",
        "metrics_q8\n"
      ],
      "metadata": {
        "id": "tGeA3HQpjgBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import patsy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def tls_fit_from_formula(formula, data):\n",
        "\n",
        "    y, X = patsy.dmatrices(formula, data=data, return_type='dataframe')\n",
        "    y = np.asarray(y).ravel()\n",
        "    X_mat = np.asarray(X)\n",
        "\n",
        "    # build augmented matrix [X | y]\n",
        "    Z = np.column_stack([X_mat, y])\n",
        "\n",
        "    # TLS via SVD: last right-singular vector\n",
        "    U, S, Vt = np.linalg.svd(Z, full_matrices=False)\n",
        "    v = Vt[-1, :]             # last row\n",
        "\n",
        "    v_x = v[:-1]\n",
        "    v_y = v[-1]\n",
        "    beta_tls = -v_x / v_y     # TLS coefficients\n",
        "\n",
        "    beta_tls = pd.Series(beta_tls, index=X.columns)\n",
        "    return beta_tls, X, y\n"
      ],
      "metadata": {
        "id": "iLr4M1fYjxD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vajj99RbrW0G"
      },
      "outputs": [],
      "source": [
        "# TLS on filtered train data\n",
        "beta_tls_filt, X_filt, y_filt = tls_fit_from_formula(formula_full, train)\n",
        "\n",
        "# Compare with OLS coefficients of the same model\n",
        "coef_ols_filt = m_final.params\n",
        "\n",
        "coef_compare_filt = pd.DataFrame({\n",
        "    'OLS_filtered': coef_ols_filt,\n",
        "    'TLS_filtered': beta_tls_filt\n",
        "})\n",
        "coef_compare_filt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tls_filt = X_filt @ beta_tls_filt.values\n",
        "\n",
        "mse_tls_filt = mse(y_filt, y_pred_tls_filt)\n",
        "r2_tls_filt  = r2_score(y_filt, y_pred_tls_filt)\n",
        "mse_tls_filt, r2_tls_filt\n"
      ],
      "metadata": {
        "id": "HJ7URxzZj4wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_rad_full= pd.read_csv(url)\n",
        "\n",
        "house_rad_full['log_price'] = np.log(house_rad_full['price'])\n",
        "\n",
        "\n",
        "house_rad_full['house_age'] = np.where(\n",
        "    house_rad_full['yr_renovated'] != 0,\n",
        "    2015 - house_rad_full['yr_renovated'],       # renovated → age since renovation\n",
        "    2015 - house_rad_full['yr_built']            # not renovated → age since built\n",
        ")\n",
        "\n",
        "\n",
        "train_full = house_rad_full.query('split == \"train\"').copy()\n",
        "\n",
        "# OLS on full data\n",
        "m_ols_full = smf.ols(formula_full, data=train_full).fit()\n",
        "\n",
        "# TLS on full data\n",
        "beta_tls_full, X_full, y_full = tls_fit_from_formula(formula_full, train_full)\n",
        "coef_ols_full = m_ols_full.params\n",
        "\n",
        "coef_compare_full = pd.DataFrame({\n",
        "    'OLS_full': coef_ols_full,\n",
        "    'TLS_full': beta_tls_full\n",
        "})\n",
        "coef_compare_full\n",
        "\n",
        "y_pred_tls_full = X_full @ beta_tls_full.values\n",
        "mse_tls_full = mse(y_full, y_pred_tls_full)\n",
        "r2_tls_full  = r2_score(y_full, y_pred_tls_full)\n",
        "mse_tls_full, r2_tls_full\n"
      ],
      "metadata": {
        "id": "9CCBTZfakCmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {\n",
        "    'train': train,\n",
        "    'test':  test,\n",
        "    'validation': val\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for name, df in splits.items():\n",
        "    y_true = df[target]\n",
        "    y_pred = m_final.predict(df)\n",
        "    mse_   = mse(y_true, y_pred)\n",
        "    r2_    = r2_score(y_true, y_pred)\n",
        "    rows.append({\n",
        "        'split': name,\n",
        "        'MSE': mse_,\n",
        "        'RMSE': np.sqrt(mse_),\n",
        "        'R2': r2_\n",
        "    })\n",
        "\n",
        "metrics_q10 = pd.DataFrame(rows)\n",
        "metrics_q10\n"
      ],
      "metadata": {
        "id": "Ij8gQWSykhdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySe_Y1iFqwzk"
      },
      "source": [
        "---\n",
        "\n",
        "## Machine Learning Approach\n",
        "\n",
        "\n",
        "### Question 11\n",
        "Apply machine learning-based linear regression methods, such as Ridge Regression, Lasso, or Elastic Net, to the dataset. Use the train-test split to evaluate model performance and focus on feature selection. Identify the most relevant features based on these methods and compare how the selected features impact the model's predictive performance. Discuss how regularization affects feature selection and the trade-offs between bias and variance in the context of house price prediction. Additionally, evaluate the stability of selected features across different methods and provide recommendations for choosing the optimal feature set.\n",
        "\n",
        "Use\n",
        "* Standardization of regressors (and possibly use of pipelines).\n",
        "* Hyperparameter tuning via cross‑validation (e.g. grid search over λ/α).\n",
        "* Compare OLS vs ridge vs lasso on the same train/test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# parse RHS of formula_full: \"price ~ x1 + x2 + ...\" -> [\"x1\", \"x2\", ...]\n",
        "rhs = formula_full.split('~', 1)[1]\n",
        "predictors = [s.strip() for s in rhs.split('+')]\n",
        "\n",
        "X_train = train[predictors].values\n",
        "y_train = train['price'].values\n",
        "\n",
        "X_test  = test[predictors].values\n",
        "y_test  = test['price'].values\n"
      ],
      "metadata": {
        "id": "fUw3EAyomdbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kse02wVrWfK"
      },
      "outputs": [],
      "source": [
        "def eval_model(name, model, X_train, y_train, X_test, y_test):\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test  = model.predict(X_test)\n",
        "\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test  = mean_squared_error(y_test,  y_pred_test)\n",
        "    r2_train  = r2_score(y_train, y_pred_train)\n",
        "    r2_test   = r2_score(y_test,  y_pred_test)\n",
        "\n",
        "    return {\n",
        "        'model': name,\n",
        "        'train_MSE': mse_train,\n",
        "        'train_RMSE': np.sqrt(mse_train),\n",
        "        'train_R2': r2_train,\n",
        "        'test_MSE': mse_test,\n",
        "        'test_RMSE': np.sqrt(mse_test),\n",
        "        'test_R2': r2_test\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OfSAN_4mfr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}